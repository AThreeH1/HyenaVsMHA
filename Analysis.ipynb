{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Assigning Values\n",
        "device = torch.device(\"cuda\")\n",
        "input_dim = 512\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "batch_size = 1000\n",
        "sequence_length = 128\n",
        "train_ratio = 0.8\n",
        "\n",
        "# Initialising random input\n",
        "x = torch.randn((batch_size, sequence_length, input_dim), requires_grad=True).to(device)\n",
        "\n",
        "# Generate training data\n",
        "torch.manual_seed(123)\n",
        "model_mha = MultiheadAttention(input_dim, d_model, num_heads).to(device)\n",
        "out = model_mha.forward(x)\n",
        "out = out.detach()\n",
        "x = x.detach()\n",
        "\n",
        "# for i in [2,4]:\n",
        "\n",
        "# Initialize HyenaOperator model\n",
        "hyena_model = HyenaOperator(d_model=d_model, l_max=sequence_length, order=4, dropout=0.0, filter_dropout=0.0)\n",
        "# hyena_model.prepare_data()\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(monitor='val_accuracy', mode='max')\n",
        "\n",
        "# initiate wandb logger\n",
        "wandb_logger = WandbLogger(project = 'Hyenalight',log_model=\"all\")\n",
        "\n",
        "# Initialize Trainer and start training\n",
        "trainer = pl.Trainer(\n",
        "    accelerator=\"gpu\",\n",
        "    max_epochs=10,\n",
        "    logger = wandb_logger,\n",
        ")\n",
        "trainer.fit(hyena_model)\n",
        "trainer.test()\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8ca3bc4defd94665a61568a6c36b3a15",
            "82ba880788154e0e9bfe02e957a2d4dd",
            "1a7ce0160d64485d8d3eae1cfcbf1d3e",
            "5a76f02592ed448397481339b7c108a4",
            "09eb8315ba43466ebd5745616d4cee89",
            "1c710126bcc44bf9b16d107c1d1af0e5",
            "403576f6617349afb8bc4e02c3bb9efb",
            "49ad7bf678f046cbb4432ccd091b281c",
            "fce77f6bca4a407d9df07e1f204c277c",
            "98436fe22d4247f592b6ea6a0f96d690",
            "6d11410d445840c58c6fb21e833731ed",
            "a3407384ca8a4d79852dfd9819b51471",
            "031317d8206547239b5f5137fe655719",
            "7b366994e2a44cac880de858432dd1e3",
            "02172422535343808f3fb825580403a9",
            "e47de06731a04465a43ba41be421d160",
            "eb913133b63b4688b65c12bac74ba966",
            "e936178b615643c9b2656d38c0b83884",
            "1690d0c07a9d42f7b472dff05f5394d1",
            "786ecc314f5c498380e7bae938b1c054",
            "122bc728576c46ba88852bc0eb249d51",
            "0e5cb9c7d6934a6685149b2a2482290f",
            "ecc59bdcbca14d478d08560c1b3873e1",
            "7ade1e72839b4289a59b9283f7a63d8f",
            "65b7940881dc45528724c9a864a1132e",
            "56c4b3d82f1842daa03eba7a98e60e6d",
            "682cfb768b1549fa96cc51b632f7b040",
            "c8bac0547ea54a39bb07295937af48f1",
            "b990dcac9f7442e49b3608858eeb6a12",
            "c117d2179ae64f18b564f102cea076c6",
            "47f2615b442349e6ba31f0b418c22e26",
            "dc13d3ae1ed54ab29fab6abf4e965ce6",
            "b9c05a218e5541b4b659de33a4399878",
            "f52cde2d89994971b4bfc496ea8fbd4c",
            "6f340a21042444e1baf265861e6bbbb3",
            "567e9faff456405ebf6035a8547287d6",
            "7c4781c82c3846108fc7b8bdd5d6fa83",
            "88a1aea3a55f424586317a168ed6c878"
          ]
        },
        "id": "AQbp4ESTHi1g",
        "outputId": "04f2f635-bf43-447f-ed21-7ec1e6e02e35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x.size(): torch.Size([1000, 128, 512])\n",
            "qkv.size(): torch.Size([1000, 128, 1536])\n",
            "qkv.size(): torch.Size([1000, 128, 8, 192])\n",
            "qkv.size(): torch.Size([1000, 8, 128, 192])\n",
            "q size: torch.Size([1000, 8, 128, 64]), k size: torch.Size([1000, 8, 128, 64]), v size: torch.Size([1000, 8, 128, 64]), \n",
            "values.size(): torch.Size([1000, 8, 128, 64]), attention.size:torch.Size([1000, 8, 128, 128]) \n",
            "values.size(): torch.Size([1000, 128, 512])\n",
            "out.size(): torch.Size([1000, 128, 512])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111365427777855, max=1.0)…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8ca3bc4defd94665a61568a6c36b3a15"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>./wandb/run-20240122_070506-hdf0va37</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ana-avhale/Hyenalight/runs/hdf0va37' target=\"_blank\">sparkling-gorge-2</a></strong> to <a href='https://wandb.ai/ana-avhale/Hyenalight' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/ana-avhale/Hyenalight' target=\"_blank\">https://wandb.ai/ana-avhale/Hyenalight</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/ana-avhale/Hyenalight/runs/hdf0va37' target=\"_blank\">https://wandb.ai/ana-avhale/Hyenalight/runs/hdf0va37</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name         | Type        | Params\n",
            "---------------------------------------------\n",
            "0 | dropout      | Dropout     | 0     \n",
            "1 | in_proj      | Linear      | 1.3 M \n",
            "2 | out_proj     | Linear      | 262 K \n",
            "3 | short_filter | Conv1d      | 10.2 K\n",
            "4 | filter_fn    | HyenaFilter | 108 K \n",
            "---------------------------------------------\n",
            "1.7 M     Trainable params\n",
            "0         Non-trainable params\n",
            "1.7 M     Total params\n",
            "6.780     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=1` in the `DataLoader` to improve performance.\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py:293: The number of training batches (40) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fce77f6bca4a407d9df07e1f204c277c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: `Trainer.fit` stopped: `max_epochs=10` reached.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=10` reached.\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:145: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.test(ckpt_path='best')` to use the best model or `.test(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n",
            "INFO: Restoring states from the checkpoint path at ./Hyenalight/hdf0va37/checkpoints/epoch=9-step=400.ckpt\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Restoring states from the checkpoint path at ./Hyenalight/hdf0va37/checkpoints/epoch=9-step=400.ckpt\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: Loaded model weights from the checkpoint at ./Hyenalight/hdf0va37/checkpoints/epoch=9-step=400.ckpt\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Loaded model weights from the checkpoint at ./Hyenalight/hdf0va37/checkpoints/epoch=9-step=400.ckpt\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=1` in the `DataLoader` to improve performance.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Testing: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "786ecc314f5c498380e7bae938b1c054"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.0011842150706797838  \u001b[0m\u001b[35m \u001b[0m│\n",
              "└───────────────────────────┴───────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.0011842150706797838   </span>│\n",
              "└───────────────────────────┴───────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='155.411 MB of 194.264 MB uploaded\\r'), FloatProgress(value=0.8000011998003815, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "47f2615b442349e6ba31f0b418c22e26"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▂▂▃▃▄▄▅▅▅▆▆▇▇▇▇█</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_loss_epoch</td><td>█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▇▁█▂▄█▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>test_loss</td><td>0.00118</td></tr><tr><td>train_loss_epoch</td><td>0.00119</td></tr><tr><td>train_loss_step</td><td>0.00118</td></tr><tr><td>trainer/global_step</td><td>400</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">sparkling-gorge-2</strong> at: <a href='https://wandb.ai/ana-avhale/Hyenalight/runs/hdf0va37' target=\"_blank\">https://wandb.ai/ana-avhale/Hyenalight/runs/hdf0va37</a><br/>Synced 5 W&B file(s), 0 media file(s), 10 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240122_070506-hdf0va37/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}